{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "PXtCqPUeHUGd",
        "1WmzUBJj5D6i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Klassifikation\n",
        "In diesem Notebook erstellen wir ein einfaches Neuronales Netzwerk mithilfe von pytorch um den MNIST Datensatz zu Klassifizieren. Dabei gehen wir entsprechend der [good practice des Deep Learning nach Kaparthy](https://karpathy.github.io/2019/04/25/recipe/) vor.\n",
        "\n",
        "1. Data Exploration\n",
        "2. Model & Traininsloop erstellen\n",
        "3. Evaluieren\n",
        "4. Overfitten\n",
        "5. Regularisieren\n",
        "6. Optimieren"
      ],
      "metadata": {
        "id": "_Xl6CxGgx2Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Exploration\n",
        "\n",
        "Als erstes werden wir die Daten erkunden, um wertvolle Erkenntnisse aus den vorliegenden Daten zu gewinnen. Dabei werden wir verschiedene Analysetechniken und Visualisierungsmethoden einsetzen, um ein umfassendes Verständnis des Datensatzes zu entwickeln. Ziel ist es, Muster, Trends und Zusammenhänge innerhalb der Daten zu identifizieren und darauf aufbauend fundierte Entscheidungen treffen zu können, zB welche Informationen den Daten entnommen werden können und welches Modell sich dazu eignet."
      ],
      "metadata": {
        "id": "PXtCqPUeHUGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zunächst laden wir die standardmässig verwendeten Module und den Datensatz."
      ],
      "metadata": {
        "id": "6QxIUqCsHc-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "# Lade den MNIST-Datensatz\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
      ],
      "metadata": {
        "id": "KIYtpMqaHQ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Inputdaten befinden sich nun in\n",
        "\n",
        "\n",
        "```\n",
        "mnist.data\n",
        "```\n",
        " Die Klassenlabel in\n",
        "\n",
        " ```\n",
        " mnist.target\n",
        " ```\n"
      ],
      "metadata": {
        "id": "GevFgv-_JfTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aufgabe 1.1\n",
        "### Welches Format haben die Daten?\n",
        "Wir betrachten das Format der Daten. Bestimmen Sie dazu\n",
        "1. die Grösse des Datensatzes: N_data\n",
        "2. die Dimension der einzelnen Input-Daten: size_input\n",
        "3. den Container des Input Daten: container_input\n",
        "4. den Input datentyp: dtype_input\n",
        "5. den Datentyp der Label: dtype_output\n",
        "6. den Wertebereich der Input-Daten: mini, maxi\n",
        "7. die möglichen Outputwerte: labels_list"
      ],
      "metadata": {
        "id": "V6_CQUbLI31-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "# Grösse des Datensatzes\n",
        "N_data, size_input = mnist.data.shape\n",
        "# Input container\n",
        "container_input = type(mnist.data[0])\n",
        "# Datentypen\n",
        "dtype_input = mnist.data.dtype\n",
        "dtype_output = type(mnist.target[0])\n",
        "# Wertebereich\n",
        "mini, maxi = mnist.data.min(), mnist.data.max()\n",
        "labels_list = np.unique(mnist.target)\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"Format des Datensatzes:\")\n",
        "print(\"Anzahl Datenpunkte:\", N_data)\n",
        "print(\"Dimension der Input Daten:\", size_input)\n",
        "print(\"Input Container:\", container_input)\n",
        "print(\"Input Datentyp:\", dtype_input)\n",
        "print(\"Output Datentyp:\", dtype_output)\n",
        "print(\"Input Wertebereich:\", f\"[{mini},{maxi}]\")\n",
        "print(\"Output Werte:\", f\"{labels_list}\")"
      ],
      "metadata": {
        "id": "x7NsBd3nI4NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Welche Daten enthält der Datensatz?\n",
        "\n",
        "Nun betrachten wir ein paar Datenbeispiele. Bei den Daten handelt es sich um Bilder mit einem string label. Wir plotten ein paar dieser Bilder zusammen mit deren Label."
      ],
      "metadata": {
        "id": "mHXsJsfnB8d8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPDKlwptG1lK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot 25 example images with labels as titles\n",
        "fig, axs = plt.subplots(5, 5, figsize=(10, 10))\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    ax.imshow(mnist.data[i].reshape(28, 28), cmap='gray')\n",
        "    ax.set_title(f\"label = {mnist.target[i]}\")\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bei den Daten handelt es sich um Schwarz-Weiss Bilder von handgeschriebenen Ziffern. Das Label gibt an, welche Ziffer im Bild geschrieben steht."
      ],
      "metadata": {
        "id": "txrLk-qUIODF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aufgabe 1.2\n",
        "### Welche Klassen gibt es und wie sind diese Verteilt?\n",
        "Nun betrachten wir die Verteilung der Klassen. Berechnen und plotten Sie dazu das Histogram der Label."
      ],
      "metadata": {
        "id": "yYjVODKaIdBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "# NumPy Histogramm-Berechnung\n",
        "counts = np.bincount(mnist.target.astype(int))\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "\n",
        "# Histogramm-Plot\n",
        "plt.bar(labels_list, counts, edgecolor='black')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Anzahl')\n",
        "plt.title('Histogramm der Datenverteilung')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "Nop7DLAUG3X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Daten sind in etwa gleichmässig verteilt und bedürfen diesbezüglich keiner weiteren Bearbeitung.\n",
        "Wenn die Klassen ungleichmässig verteilt sind, beeinflusst das die Performance eines darauf trainierten Neuronalen Netzes."
      ],
      "metadata": {
        "id": "iMuOAJ-OMgRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aufgabe 1.3\n",
        "### Wie machen wir die Klassen dem Model verständlich?\n",
        "Die Label Information liegt als String vor. Ein Neuronales Netz arbeitet jedoch in aller Regel nur mit Floats.\n",
        "Ein einfaches übersetzen der Strings in die Floatwerte 0 - 9 hätte zur Folge, dass zB ein Output von 4 näher am Zielwert 5 läge als ein Output von 1. Die 4 ist der 5 aber nicht ähnlicher als die 1.\n",
        "Es ist daher sinnvoll, jedes mögliche Label durch einen eigenen Floatwert 0 - 1 darzustellen, zB 3 -> [0,0,0,1,0,0,0,0,0,0].\n",
        "Dieser Vorgang heisst **One-Hot-Encoding** und sorgt für die saubere Trennung der Label. Ausserdem erlaubt dieses Vorgehen mit Labeln zu arbeiten, die keine Zahlenwerte sind, wie zB \"Hund\", \"Katze\", \"Maus\".\n",
        "\n",
        "Überführen Sie nun die Targetlabel in das One-Hot-Encoding Format.\n"
      ],
      "metadata": {
        "id": "V7ERDreJM5g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "# One-Hot-Encoding der Labels\n",
        "encoder = OneHotEncoder()\n",
        "labels = encoder.fit_transform(mnist.target.reshape(-1, 1))\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "\n",
        "print(\"Shape der One-Hot-Encdoded Labels:\", labels.shape)"
      ],
      "metadata": {
        "id": "F8v5YopXBStS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die shape der labels sollte nun (70000, 10) sein."
      ],
      "metadata": {
        "id": "n6iR1CRjPrLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aufgabe 1.4\n",
        "### Welche Skalierung der Daten ist sinnvoll?\n",
        "Die Input Werte sind zwar schon im float Format, allerdings skalieren Sie von 0-255. Die Erfahrung zeigt, dass Neuronale Netze am besten mit Werten zwischen -1 und 1 bzw zwischen 0 und 1 arbeiten.\n",
        "\n",
        "Reskalieren Sie die Input Daten mithilfe des Min-Max Scaling:\n",
        "$$scaled\\_data = \\frac{{data - \\min(data)}}{{\\max(data) - \\min(data)}} \\cdot 2 - 1\n",
        "$$"
      ],
      "metadata": {
        "id": "-UvO3iiaP04n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Skalierung der Bilddaten auf [-1, 1]\n",
        "mini, maxi = np.min(mnist.data), np.max(mnist.data)\n",
        "print(\"Min/Max vor Skalierung:\", mini, maxi)\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "scaled_data = 2*(mnist.data.astype(float) - mini) / (maxi - mini) - 1\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "print(\"Min/Max nach Skalierung:\", np.min(scaled_data), np.max(scaled_data))\n"
      ],
      "metadata": {
        "id": "z1hIR-IvBSqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Modell & Trainingsloop erstellen\n",
        "In diesem Abschnitt erstellen wir das Modell und die Trainingsloop. Damit definieren wir das Ziel (die zu minimierende Loss Funktion) und den Algorithmus (Model), um dieses Ziel zu erreichen."
      ],
      "metadata": {
        "id": "1WmzUBJj5D6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stellen Sie zuerst sicher, dass Sie Zugriff auf GPU ressourcen haben, auf denen die Berechnungen bis zu 20x schneller laufen.\n",
        "\n",
        "Zum aktivieren in Google Colab:\n",
        "\n",
        "Runtime -> Change Runtime\n",
        "\n",
        "Hardware Accelerator -> GPU"
      ],
      "metadata": {
        "id": "4diSF3YkN6Ob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Prüfen, ob GPU verfügbar ist. Sonst Berechnung auf CPU. Nur NVIDIA GPUs mit CUDA werden unterstützt\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device, \" (= cuda ?)\")"
      ],
      "metadata": {
        "id": "ffxxOe5DETfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um auf der GPU zu rechnen, müssen sich sowohl das Model als auch die Trainingsdaten im GPU Speicher befinden. Das erreicht man mit\n",
        "```\n",
        "model.to(device)\n",
        "```\n",
        "bzw\n",
        "```\n",
        "data = data.to(device)\n",
        "```"
      ],
      "metadata": {
        "id": "GzvzDD6thJNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zuerst laden wir den Datensatz als Tensor, mit dem das Model rechnen wird. Wir unterteilen den Datensatz in Trainings- und Testdaten. Dabei wird has Modell so trainiert, dass die Trainingsdaten möglichst gut approximiert werden. Die Performance des fertig trainierten Modells auf neue Daten wird dann mit dem Testdatensatz abgeschätzt."
      ],
      "metadata": {
        "id": "h5YtsiFi8Fk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Laden des MNIST-Datensatzes als pytorch tensoren\n",
        "transform = transforms.ToTensor()\n",
        "training_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "1qFRUtrYEgpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aufgabe 2.1\n",
        "\n",
        "Hier definieren wir das Modell.\n",
        "Als Klassifizierungsmodell nehmen wir ein einfaches Multi-Layer Perceptron (MLP), welches zu einem gegebenen MNIST Bild die Wahrscheinlichkeit ausgibt, mit der es die Ziffern 0-9 darstellt. Der höchste Score entspricht also der zugeordneten Ziffer.\n",
        "Erstellen Sie ein MLP mit den folgenden Spezifikationen:\n",
        "* Input: B/W-Bilder mit 28x28 pixeln.\n",
        "* 3 hidden layer (fc1 - fc3, fully connected / linear): 64 Knoten, Aktivierung: ReLU\n",
        "* output layer (fc4): 10 Knoten, Aktivierung: keine\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kbVhjxd9PnDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "# Definieren des Models\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Multi-Layer Perceptron (MLP) Neural Network.\n",
        "\n",
        "        This class represents a simple MLP model with 4 fully connected layers.\n",
        "        The input to the network should have a shape of (N_batch, 28*28), where N_batch is the batch size\n",
        "        and 28*28 is the flattened input image size (28x28 pixels).\n",
        "\n",
        "        The network architecture:\n",
        "        - Input layer: Fully connected layer with 64 output units and ReLU activation.\n",
        "        - Hidden layers: Two fully connected layers with 64 output units and ReLU activation for each.\n",
        "        - Output layer: Fully connected layer with 10 output units (representing 10 classes).\n",
        "\n",
        "        The forward() method defines how the input tensor flows through the layers of the network.\n",
        "\n",
        "        Example usage:\n",
        "        >>> model = MLP()\n",
        "        >>> input_data = torch.randn(32, 28*28)  # Assuming batch size of 32\n",
        "        >>> output = model(input_data)\n",
        "\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # make sure shape is (N_batch,N_pixel)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# Alternative methode, die nicht einfach erlaubt den output jedes layers einzusehen\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # make sure shape is (N_batch,N_pixel)\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Das selbe Model koennen wir auch kurz direkt definieren (ausgenommen das reshaping im forward)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(28*28, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64, 10)\n",
        ")\n",
        "'''\n",
        "model = MLP()"
      ],
      "metadata": {
        "id": "mYZb1_x5EtS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aufgabe 2.2\n",
        "Nun erstellen wir die Trainingsloop, in der das Modell einmal auf allen gegebenen Daten trainiert wird.\n",
        "Vervollständigen Sie die folgende Trainingsloop, indem sie mit dem  model die Vorhersage berechnen und mithilfe der *criterion* funktion den loss berechnen, der die Vorhersage mit den tatsächlichen Labeln vergleicht.\n",
        "\n",
        "Nachdem der Loss berechnet wurde, können damit die Gradienten für das Parameterupdate im Backward-Prozess berechnet werden:\n",
        "\n",
        "```\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "Die entsprechenden Updates der Gewichte werden dann mit dem gewählten optimizer Algorithmus angewendet:\n",
        "```\n",
        "optimizer.step()\n",
        "```\n",
        "\n",
        "\n",
        "Im Deep Learning sind die Datensätze zu gross, als dass ein Model auf allen Daten gleichzeitig trainieren könnte. Deswegen verwenden wir einen Dataloader, der den Datensatz in viele kleine Bündel (batches) aufteilt, die dann eins nach dem anderen geladen und das Modell darauf trainiert wird. Die Reihenfolge der Daten sollte während des Trainings in jeder Epoche zufällig gewählt werden."
      ],
      "metadata": {
        "id": "EmI-bKBlfIuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Trainingsloop\n",
        "def train(model: nn.Module,\n",
        "          criterion: nn.Module,\n",
        "          optimizer: optim.Optimizer,\n",
        "          data_loader: DataLoader,\n",
        "          show_progress: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Train the given model using the provided data_loader and optimization parameters.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to be trained.\n",
        "        criterion: The loss function used for optimization.\n",
        "        optimizer: The optimization algorithm (e.g., SGD, Adam) for updating model parameters.\n",
        "        data_loader (DataLoader): The DataLoader providing the training data in batches.\n",
        "        show_progress (bool, optional): If True, show a progress bar during training. Default is True.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Example usage:\n",
        "        >>> model = MLP()\n",
        "        >>> criterion = nn.CrossEntropyLoss()\n",
        "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "        >>> train(model, criterion, optimizer, train_data_loader, show_progress=True)\n",
        "    \"\"\"\n",
        "    model.train() # make sure model is in training mode\n",
        "    for images, labels in tqdm(data_loader, desc=\"Train\", disable=not show_progress):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, labels)\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "\n",
        "        optimizer.zero_grad() # remove all gradients computed before\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "aO0lKO2pE5-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Um den Trainingsprozess nachzuverfolgen, betrachten wir die Entwicklung des Loss über die Epochen hinweg, sowohl für das Trainings als auch das Testset.\n",
        "Um den Stand des Models nach einer Trainingsepoche festzustellen, wird der Loss idealerweise nach beenden des Trainings auf dem gesamten Test oder Trainingsdatensatz berechnet. In der Praxis wird um Ressourcen zu sparen der Trainingloss meist schon während des Trainings gesammelt, was allerdings keine saubere Bewertung der Performance nach der Trainingsepoche ist. Da die Performance aber massgeblich auf dem Testset ermittelt wird, ist dieser Trade-Off berechtigt. Allerdings gilt zu beachten, dass der Loss in diesem Fall nicht stetig sinkt.\n",
        "\n",
        "Wenn kein Training stattfindet, braucht der Gradient für das Parameterupdate nicht berechnet werden, was einen Grossteil der Rechenressourcen ausmacht.\n",
        "Mit *torch.no_grad()* kann diese Berechnung unterbunden und Ressourcen geschont werden."
      ],
      "metadata": {
        "id": "MGEAyhzfinXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Berechnung des Losses auf einem Dataset, ohne Training des Models\n",
        "def compute_average_loss(model: nn.Module,\n",
        "                         criterion: nn.Module,\n",
        "                         data_loader: DataLoader,\n",
        "                         show_progress: bool = True) -> float:\n",
        "    \"\"\"\n",
        "    Compute the average loss on the given dataset using the provided model and loss function.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model used for computing the loss.\n",
        "        criterion (nn.Module): The loss function used for calculating the loss.\n",
        "        data_loader (DataLoader): The DataLoader providing the dataset in batches.\n",
        "        show_progress (bool, optional): If True, show a progress bar during computation. Default is True.\n",
        "\n",
        "    Returns:\n",
        "        float: The average loss per image in the dataset.\n",
        "\n",
        "    Example usage:\n",
        "        >>> model = MLP()\n",
        "        >>> criterion = nn.CrossEntropyLoss()\n",
        "        >>> average_loss = compute_average_loss(model, criterion, validation_data_loader, show_progress=True)\n",
        "    \"\"\"\n",
        "    model.eval() # put model in evaluation mode, where it works as intended for its application\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader, desc=\"Compute Loss\", disable=not show_progress):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "\n",
        "            total_loss += loss.item() * images.size(0)  # loss.item() gibt den floatwert des losses aus. loss ist noch ein tensor\n",
        "            total_samples += images.size(0)\n",
        "\n",
        "    average_loss = total_loss / total_samples # average loss per image\n",
        "    return average_loss"
      ],
      "metadata": {
        "id": "P_62_snaFANV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aufgabe 2.3\n",
        "\n",
        "Nun testen wir Model und Trainingsloop indem wir für zwei Epochen trainieren um zu testen, ob der Trainingsloss sich verringert.\n",
        "Vervollständigen Sie den Code, indem Sie die Loss Funktion *criterion*, den *optimizer* Algorithmus sowie den *Dataloader* fürs Training definieren.\n",
        "\n",
        "Verwenden Sie Cross Entropy loss, den Adam Optimizer mit Defaultwerten, und einen Dataloader mit batchsize 256, der die Trainingsdaten in jedem durchlauf neu mischt."
      ],
      "metadata": {
        "id": "FCBexkp7k16V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Testen des Modells und des Trainingsloops\n",
        "model = MLP()\n",
        "model.to(device)\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "training_loader = DataLoader(training_data, batch_size=256, shuffle=True)\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "for i in range(2):  # Trainieren für 2 Epochen\n",
        "    train(model, criterion, optimizer, training_loader)\n",
        "    loss = compute_average_loss(model, criterion, training_loader)\n",
        "    print(f\"\\nepoch {i}: {loss:.4f}\\n\")"
      ],
      "metadata": {
        "id": "vZE1FyZP3NfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Der Loss sollte mit jeder Trainingsepoche sinken"
      ],
      "metadata": {
        "id": "EmiDFSPFCjwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluieren\n",
        "In diesem Abschnitt erstellen wir eine Methode zur Bewertung des Modells.\n",
        "Um die Leistung eines Modells zu bewerten, muss eine passende Metrik gewählt werden, welche die Erfüllung des Ziels nachvollziehbar misst.\n"
      ],
      "metadata": {
        "id": "Z9La9BjQ4ML5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In der Klassifikation ist die naheliegendste Metrik die Accuracy, welche den Anteil korrekter Klassifikationen zeigt\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Anzahl korrekter Klassifizierungen}}{\\text{Anzahl Datenpunkte}}$$\n",
        "\n",
        "Allerdings ist die Accuracy mit Vorsicht zu geniessen, insbesondere bei unbalancierten Datensätzen. Beispiel: 90% Klasse 1, 10% Klasse 0. Model sagt immer 1 und bekommt damit 90% accuracy.\n",
        "Ausserdem liefert diese Metrik bei mehreren Klassen keine Einsicht, welche Klassen problematischer sind als andere.\n",
        "\n",
        "\n",
        "Die Alternative dazu ist die Konfusionsmatrix, welche eine genauere Darstellung über die korrekte Zuordnung erlaubt.\n",
        "\n",
        "![Konfusionsmatrix](https://www.researchgate.net/publication/336402347/figure/fig3/AS:812472659349505@1570719985505/Calculation-of-Precision-Recall-and-Accuracy-in-the-confusion-matrix.ppm)\n",
        "\n",
        "Aus Precision, dem Anteil richtiger Daten die erkannt wurden, und Recall, dem Anteil erkannter Daten die tatsächlich richtig sind, lässt sich der F1-score berechnen, welcher beide Aspekte gleichermassen berücksichtigt\n",
        "\n",
        "$$ F1 = 2\\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "Alle vier Grössen können Werte zwischen 0 und 1 bzw 0 % und 100 % annehmen, wobei 100 % ein perfektes Ergebnis darstellt.\n",
        "\n",
        "\n",
        "Berechnen Sie mit den richtigen sowie den verhorgesagten Labeln den *F1-Score* sowie die *Konfusionsmatrix*"
      ],
      "metadata": {
        "id": "_cHet7ee3oa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             data_loader: DataLoader,\n",
        "             show_progress: bool = True) -> tuple:\n",
        "    \"\"\"\n",
        "    Evaluate the given model on the provided data_loader and compute the confusion matrix and F1 score.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to be evaluated.\n",
        "        data_loader (DataLoader): The DataLoader providing the evaluation data in batches.\n",
        "        show_progress (bool, optional): If True, show a progress bar during evaluation. Default is True.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the confusion matrix (as a 2D array) and the F1 score (as a float).\n",
        "\n",
        "    Example usage:\n",
        "        >>> model = MLP()\n",
        "        >>> test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "        >>> conf_mat, f1 = evaluate(model, test_loader)\n",
        "        >>> print(f'F1 score: {f1}')\n",
        "        >>> plot_confusion_matrix(conf_mat)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(data_loader, desc=\"Evaluate\", disable=not show_progress):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_preds.append(preds.cpu())\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    all_preds = torch.cat(all_preds)\n",
        "\n",
        "\n",
        "##############################################\n",
        "############ ENTER YOUR CODE HERE ############\n",
        "##############################################\n",
        "\n",
        "    conf_mat = ...\n",
        "    f1 = ...\n",
        "\n",
        "##############################################\n",
        "##############################################\n",
        "##############################################\n",
        "\n",
        "    return conf_mat, f1\n",
        "\n",
        "def plot_confusion_matrix(conf_mat: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Plot the confusion matrix using a heatmap.\n",
        "\n",
        "    Args:\n",
        "        conf_mat (np.ndarray): The confusion matrix as a 2D numpy array.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    Example usage:\n",
        "        >>> conf_mat = np.array([[10, 2, 3], [1, 15, 1], [2, 2, 8]])\n",
        "        >>> plot_confusion_matrix(conf_mat)\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted labels\")\n",
        "    plt.ylabel(\"True labels\")\n",
        "\n",
        "# Testen der Evaluationsmethode\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
        "model = MLP().to(device)\n",
        "conf_mat, f1 = evaluate(model, test_loader)\n",
        "print(f'F1 score: {f1}')\n",
        "plot_confusion_matrix(conf_mat)"
      ],
      "metadata": {
        "id": "C_IKqQNx3U04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Für mehrere Klassen zeigt die Konfusionsmatrix, wie oft eine Klasse richtig zugeordnet wird bzw mit welcher anderen Klasse sie wie oft verwechselt wird."
      ],
      "metadata": {
        "id": "8n6EQbGavPlU"
      }
    }
  ]
}